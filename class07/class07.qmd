---
title: "Class 7: Machine Learning 1"
author: "Nundini Varshney (PID: A16867985)"
format: html
---

# Clustering Methods 

The broad goal here is to find groupings (clusters) in your input data. 

## Kmeans 

First, let's make up some data to cluster.

```{r}
x <- rnorm(1000)
hist(x)
```

Make a vector of length 60 with 30 points centered at -3 and 30 points centered at +3
```{r}
tmp <- c(rnorm(30, mean=-3), rnorm(30, mean=3))
tmp
```

I will now make a wee x and y dataset with 2 groups of points. 


```{r}
x <- cbind(x=tmp, y=rev(tmp))
x
plot(x)
```

```{r}
k <- kmeans(x, centers=2)
k
```

> Q. From your result object `k` how many points are in each cluster?

```{r}
k$size
```

> Q. What "component" of your result object details the cluster membership?

```{r}
k$cluster
```


> Q. Cluster centers?

```{r}
k$centers
```

> Q. Plot of our clustering results

```{r}
plot(x, col=k$cluster)
points(k$centers, col="blue", pch=15, cex=2)
```

We can cluster into 4 groups 

```{r}
# kmeans 
k4 <- kmeans(x, centers=4)
# plot results
plot(x, col=k4$cluster)
```

A big limitation of kmeans is that it does what you ask even if you ask for silly clusters. 

## Hierarchical Clustering 

The main base R function for Hierarchical Clustering is `hclust()`.
Unlike `kmeans()` you can not just pass it your data as input. You first need to calculate a distance matrix.

```{r}
d <- dist(x)
hc <- hclust(d)
hc
```

Use `plot()` to view results.
```{r}
plot(hc)
abline(h=10, col="red")
```

To make the "cut" and get our cluster membership vector, we can use the `cutree()` function.

```{r}
grps <- cutree(hc, h=10)
grps
```

Make a plot of our data colored by hclust results
```{r}
plot(x, col=grps)
```

# Principal Component Analysis (PCA)

Here we will do Principal Component Analysis (PCA) on some food 

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names = 1)
head(x)
```


```{r}
#rownames(x) <- x[,1]
#x <- x[, -1]
#x
```

Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?
```{r}
dim(x)
```
Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

Second approach because if you run the first code multiple times, it keeps eliminating the first column.


```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```
Q3: Changing what optional argument in the above barplot() function results in the following plot?

The beside argument


Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?
```{r}
pairs(x, col=rainbow(10), pch=16)
```
 
## PCA to the rescue 

The main "base" R function for PCA is called `prcomp()`. Here we need to take the transpose of our input.

```{r}
pca <- prcomp( t(x) )
summary(pca)

```
 > Q. How much variance is captured in 2 PCs

96.5%

To make our main "PC score plot" (a.k.a. "PC1 vs PC2 plot", or "PC plot", or "ordination plot").

```{r}
attributes(pca)
```

We are after the `pca$x` result component to make our main PCA plot. 

```{r}
pca$x
```

```{r}
mycols <- c("orange", "red", "blue", "darkgreen")
plot(pca$x[,1], pca$x[,2], col=mycols, pch=16, xlab="PC1 (67.4%)", ylab="PC2 (29%)")
```

Another important result from PCA is how the original variables (in this case, the foods) contribute to the PCs.  

This is contained in the `pca$rotation` object - folks often call this the "loadings" or "contributions" to the PCs

```{r}
pca$rotation
```

We can make a plot along PC1

```{r}
library(ggplot2)

contrib <- as.data.frame(pca$rotation)

ggplot(contrib) + 
  aes(PC1, rownames(contrib)) +
  geom_col()
```



